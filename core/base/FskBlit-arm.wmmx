@
@     Copyright (C) 2010-2015 Marvell International Ltd.
@     Copyright (C) 2002-2010 Kinoma, Inc.
@
@     Licensed under the Apache License, Version 2.0 (the "License");
@     you may not use this file except in compliance with the License.
@     You may obtain a copy of the License at
@
@       http://www.apache.org/licenses/LICENSE-2.0
@
@     Unless required by applicable law or agreed to in writing, software
@     distributed under the License is distributed on an "AS IS" BASIS,
@     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@     See the License for the specific language governing permissions and
@     limitations under the License.
@
@#if (__arm__)

	.text	@CODE, READONLY
	@.fpu    neon

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@                               @@@@@@@@@@@@
@@@@@@@@@@@@ our wonderful WMMX debug tool @@@@@@@@@@@@
@@@@@@@@@@@@                               @@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

	.macro BNIE_CHECK_REG_00 stop_flag, idx
			
	STMFD   sp!, {r0-r12, lr}
	
	mov     r3, sp			@core registers, but no sp
	SUB     sp, sp, #256	@give enough head room
	
	@// Now we are safe to corrupt registers
	mov	  r0, #\stop_flag
	mov   r1, #\idx

	mov   r2, sp
	mov   r2, r2, lsr #3
	mov   r2, r2, lsl #3
	add	  r2, r2, #8		@8 byte aligned
	mov	  sp, r2			@wmmx registers
	str	  r3, [sp]			@entry sp
	add	  r8, sp, #8
	
	wstrd   wr0,  [r8], #8
	wstrd   wr1,  [r8], #8
	wstrd   wr2,  [r8], #8
	wstrd   wr3,  [r8], #8
	wstrd   wr4,  [r8], #8
	wstrd   wr5,  [r8], #8
	wstrd   wr6,  [r8], #8
	wstrd   wr7,  [r8], #8
	wstrd   wr8,  [r8], #8
	wstrd   wr9,  [r8], #8
	wstrd   wr10, [r8], #8
	wstrd   wr11, [r8], #8
	wstrd   wr12, [r8], #8
	wstrd   wr13, [r8], #8
	wstrd   wr14, [r8], #8
	wstrd   wr15, [r8]
	
	MRS     r4, cpsr        @// preserve flags
	BL      my_check_reg_wmmx
	MSR     cpsr_f, r4      @// restore flags

	add		r8, sp, #8
	wldrd   wr0,  [r8], #8
	wldrd   wr1,  [r8], #8
	wldrd   wr2,  [r8], #8
	wldrd   wr3,  [r8], #8
	wldrd   wr4,  [r8], #8
	wldrd   wr5,  [r8], #8
	wldrd   wr6,  [r8], #8
	wldrd   wr7,  [r8], #8
	wldrd   wr8,  [r8], #8
	wldrd   wr9,  [r8], #8
	wldrd   wr10, [r8], #8
	wldrd   wr11, [r8], #8
	wldrd   wr12, [r8], #8
	wldrd   wr13, [r8], #8
	wldrd   wr14, [r8], #8
	wldrd   wr15, [r8]

	ldr		sp, [sp]
	LDMFD	sp!, {r0-r12,lr}

	.endm



	.macro BNIE_CHECK_MEM_00 idx, addr, offset, size
			
	STMFD   sp!, {r0-r12, lr}
	
	@// Now we are safe to corrupt registers
	mov	  r12, \addr
	mov	  r0, #\idx
	mov	  r1, r12
	mov   r2, #\offset
	mov   r3, #\size

	mov     r5, sp			@core registers, but no sp
	SUB     sp, sp, #256	@give enough head room

	mov   r6, sp
	mov   r6, r6, lsr #3
	mov   r6, r6, lsl #3
	add	  r6, r6, #8		@8 byte aligned
	mov	  sp, r6			@wmmx registers
	str	  r5, [sp]			@entry sp
	add	  r8, sp, #8
	
	wstrd   wr0,  [r8], #8
	wstrd   wr1,  [r8], #8
	wstrd   wr2,  [r8], #8
	wstrd   wr3,  [r8], #8
	wstrd   wr4,  [r8], #8
	wstrd   wr5,  [r8], #8
	wstrd   wr6,  [r8], #8
	wstrd   wr7,  [r8], #8
	wstrd   wr8,  [r8], #8
	wstrd   wr9,  [r8], #8
	wstrd   wr10, [r8], #8
	wstrd   wr11, [r8], #8
	wstrd   wr12, [r8], #8
	wstrd   wr13, [r8], #8
	wstrd   wr14, [r8], #8
	wstrd   wr15, [r8], #8
	
	MRS     r4, cpsr        @// preserve flags
	BL      my_check_mem_wmmx
	MSR     cpsr_f, r4      @// restore flags

	add		r8, sp, #8
	wldrd   wr0,  [r8], #8
	wldrd   wr1,  [r8], #8
	wldrd   wr2,  [r8], #8
	wldrd   wr3,  [r8], #8
	wldrd   wr4,  [r8], #8
	wldrd   wr5,  [r8], #8
	wldrd   wr6,  [r8], #8
	wldrd   wr7,  [r8], #8
	wldrd   wr8,  [r8], #8
	wldrd   wr9,  [r8], #8
	wldrd   wr10, [r8], #8
	wldrd   wr11, [r8], #8
	wldrd   wr12, [r8], #8
	wldrd   wr13, [r8], #8
	wldrd   wr14, [r8], #8
	wldrd   wr15, [r8], #8

	ldr		sp, [sp]
	LDMFD	sp!, {r0-r12,lr}

	.endm


	.global fillColor16_arm_wmmx
	.type fillColor16_arm_wmmx, %function
	.align 4

height	.req	r0 
drb		.req	r1 
src		.req	r2 
cc		.req	r2 
dst		.req	r3 
width0	.req	r4 
width	.req	r5 

dst0	.req	r12		@debug only

w3		.req	r6
w4		.req	r7
cccc	.req	r8

wr_dst_tmp	.req	wr0
wr_dst_shift	.req	wr1
wr_dst	.req	wr2


.equ REGIS_SHIFT,	(11*4)
.equ CACHE_SHIFT,	(0*4)
.equ SP_SHIFT,		(REGIS_SHIFT + CACHE_SHIFT)
.equ width_SHIFT,	(0*4 + SP_SHIFT)


fillColor16_arm_wmmx:
        .fnstart
        .save   {r0-r9, lr}
        stmfd   sp!, {r4-r9}

    ldr width0,  [sp, #24]
	mov		width,width0
    ldrh    cc, [src]

    TBCSTH  wr_dst, cc

fillColor16_arm_wmmx_start:
    cmp     width, #4
    blo     less_than_8_case_16

	rsb	w3, dst, #0
    ands    w3, w3, #0x7
    beq     dst_8_bytes_aligned_case_16

dst_header:
    strh    cc,[dst],#2
    subs    w3,w3,#2
    sub     width,width,#1
    bne     dst_header


dst_8_bytes_aligned_case_16:
    cmp width, #4
    blo less_than_8_case_16

    lsr w3,width,#2

dst_8_bytes_aligned_case_16_loop:
    subs    w3, w3, #1
    wstrd   wr_dst, [dst], #8
    bne dst_8_bytes_aligned_case_16_loop

less_than_8_case_16:
    ands    width,width,#3
    beq dst_end

dst_tail_loop:
    strh    cc,[dst],#2
    subs     width,width,#1
    bne     dst_tail_loop

dst_end:
	subs height, height, #1
	movne	width, width0
	add	dst, dst, drb
	bne	fillColor16_arm_wmmx_start

   ldmfd       sp!, {r4-r9}
    mov         pc,lr

    .fnend


	.global fillColor32_arm_wmmx
	.type fillColor32_arm_wmmx, %function
	.align 4

fillColor32_arm_wmmx:
	.fnstart
	stmfd	sp!, {r0-r9, lr}

	ldr	width0, [sp, #width_SHIFT]
	ldr	cc,	[src]

	mov	width0, width0, lsl #2
	mov	width, width0
	TBCSTW	wr_dst, cc

fillColor32_arm_wmmx_start:
	cmp	width, #8
	blo	less_than_8_case_32

	rsb	w3, dst, #0
	ands	w3, w3, #0x7
	beq	dst_8_bytes_aligned_case_32

	str	cc, [dst], #4
	sub	width, width, w3

dst_8_bytes_aligned_case_32:
	cmp		width, #8
	blo	less_than_8_case_32

	subs	width, width, #8
	wstrd	wr_dst, [dst], #8
	bhs	dst_8_bytes_aligned_case_32

less_than_8_case_32:
	cmp	width, #0
	beq	dst_0_bytes_case_32

	str	cc, [dst], #4	

dst_0_bytes_case_32:
	subs 	height, height, #1
	movne	width, width0
	add	dst, dst, drb
	bne	fillColor32_arm_wmmx_start

	ldmfd	sp!, {r0-r9, pc}
	
	.fnend

@=============================================================
@======== convert_32BGRA_to_32A16RGB565LE_arm_wmmx ===========
@=============================================================

.global convert_32BGRA_to_32A16RGB565LE_arm_wmmx
.type   convert_32BGRA_to_32A16RGB565LE_arm_wmmx, %function
.align 4

s		.req	r0
w		.req	r1
h		.req	r2
rb		.req	r3
		.unreq	src
src		.req	r8

@Assuming the src is double word aligned to use wldrd
@FskBitmapNewallocate aligned memory for pixels
@64 bytes for head pointer and 32 bytes for rowbytes

convert_32BGRA_to_32A16RGB565LE_arm_wmmx:
.fnstart
		STMFD		sp!,		{r4-r8}

		ldr			r4,			=0xFFF8FCF8
		wzero		wr15
		tbcstw		wr14,		r4

	ARGB_to_32A16RGB565LE_line:
		mov			src,		s
		add			s,			s,			rb
		lsrs		r4,			w,		#2
		beq			ARGB_to_32A16RGB565LE_line_tail

		wldrd		wr0,		[src]
		wldrd		wr1,		[src,#8]

	ARGB_to_32A16RGB565LE_line_4pixel:
		wand		wr0,		wr0,		wr14
		wand		wr1,		wr1,		wr14
		wunpckilb	wr2,		wr0,		wr1
		wunpckihb	wr3,		wr0,		wr1

		subs		r4,			r4,			#1

		wldrdne		wr0,		[src,#16]

		wunpckilb	wr4,		wr2,		wr3
		wunpckihb	wr5,		wr2,		wr3

		wunpckelub	wr6,		wr4
		wunpckehub	wr7,		wr4

		wunpckilb	wr8,		wr15,		wr5
		wunpckihb	wr9,		wr15,		wr5

		wsrlh		wr6,		wr6,		#3
		wsllh		wr7,		wr7,		#3

		wldrdne		wr1,		[src,#24]

		wor			wr6,		wr7,		wr6
		wor			wr6,		wr8,		wr6

		wunpckilh	wr10,		wr6,		wr9
		wunpckihh	wr11,		wr6,		wr9

		wstrd		wr10,		[src],		#8
		wstrd		wr11,		[src],		#8

		bne			ARGB_to_32A16RGB565LE_line_4pixel

	ARGB_to_32A16RGB565LE_line_tail:
		ands		r4,			w,			#3
		beq			ARGB_to_32A16RGB565LE_line_done

	ARGB_to_32A16RGB565LE_line_1pixel:
		ldrb		r5,			[src,#0]
		ldrb		r6,			[src,#1]
		ldrb		r7,			[src,#2]

		lsr			r5,			#3
		lsr			r6,			#2
		lsr			r7,			#3

		lsl			r6,			#5
		lsl			r7,			#11

		orr			r5,			r5,			r6
		orr			r5,			r5,			r7

		mov			r6,			#0
		strh		r5,			[src],		#2
		strb		r6,			[src],		#2

		subs		r4,			r4,			#1
		bne			ARGB_to_32A16RGB565LE_line_1pixel

	ARGB_to_32A16RGB565LE_line_done:
		subs		h,			#1
		bne			ARGB_to_32A16RGB565LE_line

		LDMFD		sp!, 		{r4-r8}
		mov			pc,			lr
.fnend

	.end
@#endif
