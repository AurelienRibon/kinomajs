@
@     Copyright (C) 2010-2015 Marvell International Ltd.
@     Copyright (C) 2002-2010 Kinoma, Inc.
@
@     Licensed under the Apache License, Version 2.0 (the "License");
@     you may not use this file except in compliance with the License.
@     You may obtain a copy of the License at
@
@      http://www.apache.org/licenses/LICENSE-2.0
@
@     Unless required by applicable law or agreed to in writing, software
@     distributed under the License is distributed on an "AS IS" BASIS,
@     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@     See the License for the specific language governing permissions and
@     limitations under the License.
@
#if (__arm__)
@ This file was automatically generated from
@ /home/mkellner/fsk/core/graphics/yuv420torgb565le-arm.s
@ on Wed, 28 Oct 2009 05:58:41 GMT
@	IMPORT	memcpy
@	IMPORT	parse_proc

	.text	@CODE, READONLY

	@yuv420	
	.global	my_FskCopyYUV42016RGB565SE_unity_prototype_arm_v4 @72	fps with 352x288 on 200Mhz BlackJack
	.global	my_FskCopyYUV42016RGB565SE_unity_bc_prototype_arm_v4 @63
	.global	my_FskCopyYUV42016RGB565SE_scale_prototype_arm_v4 @50
	.global	my_FskCopyYUV42016RGB565SE_scale_bc_prototype_arm_v4 @46

	.global	my_FskCopyYUV42016RGB565SE_scale_bc_prototype_arm_v5 @53
	.global	my_FskCopyYUV42016RGB565SE_unity_bc_prototype_arm_v5 @72

	.global	my_FskCopyYUV42016RGB565SE_unity_bc_dub_ver_prototype_arm_v5 @130
	.global	my_FskCopyYUV42016RGB565SE_unity_bc_dub_ver_prototype_arm_v4 @116


	.global	yuv420ito16RGB565SE_unity_bc_arm_v5 @76 fps
	.global	yuv420ito16RGB565SE_unity_bc_arm_v4 @67
		

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	PRE_LOAD_u0_end
@;;;;;;;;;;;;;;;;;;;;;;;;;;;
	ldr	u0_end, [sp, #u0_end_SHIFT_u]

	.endm


@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	MYMLA_BT_v5 op1, op2, op3, op4, scratch
@;;;;;;;;;;;;;;;;;;;;;;;;;;;
		
	@smlabt	w0, w1, Cx, w2				;uvg:		vp*102 + up*51	
	smlabt	\op1, \op2, \op3, \op4 @uvg:		vp*102 + up*51

	.endm

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	MYMLA_BT_v4 op1, op2, op3, op4, scratch
@;;;;;;;;;;;;;;;;;;;;;;;;;;;
		
	mov	\scratch, \op3, lsr #16
	mla	\op1, \op2, \scratch, \op4

	.endm

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	MYMLA_BB_v5 op1, op2, op3, op4
@;;;;;;;;;;;;;;;;;;;;;;;;;;;
		
	@smlabb	w4, w6, Cx, xBx				;yp = yp * C2 + xBx;	
	smlabb	\op1, \op2, \op3, \op4 @yp = yp * C2 + xBx;

	.endm

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	MYMLA_BB_v4 op1, op2, op3, op4
@;;;;;;;;;;;;;;;;;;;;;;;;;;;

	mov	\op1, \op3, lsl #16
	mov	\op1, \op1, lsr #16
	mla	\op1, \op2, \op1, \op4 @yp = yp * C2 + xBx;

	.endm

	.equ REGIS_SHIFT_u, (9*4)
	.equ CACHE_SHIFT_u, (2*4)
	.equ SP_SHIFT_u, (REGIS_SHIFT_u + CACHE_SHIFT_u)


y0	.req	r0 @global
u0	.req	r1 
v0	.req	r2 
dst	.req	r3 
y1	.req	r4 
xBx	.req	r5 
Cx	.req	r6 

height	.req	r7 @push to stack from here
width	.req	r8 
yrb	.req	r9 
uvrb	.req	r10 
d_stride	.req	r11 

w0	.req	r7 @scratch
w1	.req	r8 
w2	.req	r9 
w3	.req	r10 
w4	.req	r11 
w5	.req	r14 
w6	.req	r12 

	.equ u0_end_SHIFT_u, (0)
	.equ drb_SHIFT_u, (4)
	.equ Bx_SHIFT_u, (0*4 + SP_SHIFT_u)
	.equ Cx_SHIFT_u, (1*4 + SP_SHIFT_u)
	.equ height_SHIFT_u, (2*4 + SP_SHIFT_u)
	.equ width_SHIFT_u, (3*4 + SP_SHIFT_u)
	.equ yrb_SHIFT_u, (4*4 + SP_SHIFT_u)
	.equ uvrb_SHIFT_u, (5*4 + SP_SHIFT_u)
	.equ d_stride_SHIFT_u, (6*4 + SP_SHIFT_u)


@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	UNITY_BC_CONVERSION arch, version
@;;;;;;;;;;;;;;;;;;;;;;;;;;;

	stmfd	sp!,{r4-r11,lr}
	sub	sp,sp,#CACHE_SHIFT_u
		
	add	w6,sp,#Bx_SHIFT_u @
	ldmia	w6,{xBx-d_stride} 
		
	mov	w6, d_stride
	str	w6, [sp, #drb_SHIFT_u] @cache drb here
		
	add	y1, y0, yrb
	mov	yrb, yrb, lsl #1
	mov	d_stride, d_stride, lsl #1
		
	sub	yrb, yrb, width
	sub	uvrb, uvrb, width, lsr #1
	sub	d_stride, d_stride, width, lsl #1

	mov	width, width, lsr #1
	mov	height, height, lsr #1

	add	w6,sp,#height_SHIFT_u @
	stmia	w6,{height-d_stride} 

	add	w6, yrb, #1
	mov	w6, #1
	sub	y0, y0, w6
	sub	y1, y1, w6
		
	add	w6, uvrb, #1
	mov	w6, #1
	sub	u0, u0, w6
	sub	v0, v0, w6

	add	w6, d_stride, #4
	mov	w6, #4
	sub	dst, dst, w6

	add	w6, u0, width
	str	w6, [sp, #u0_end_SHIFT_u]

\version\().loop:	

	ldrb	w0, [u0, #1]! @u
	ldrb	w1, [v0, #1]! @v
	ldrb	w6, [y0, #1]! @yp 
		
	mov	w5, #31 
	mov	w2, Cx, lsr #17 @51
	sub	w0, w0, #128 @up-128
	mul	w2, w0, w2 @up*51
	sub	w1, w1, #128 @vp-128   
		
	@v5 implementation	
	@smlabt	w0, w1, Cx, w2				;uvg:		vp*102 + up*51	
	@v4 implementation	
	@mov		w4, Cx, lsr #16	
	@mla		w0, w1, w4, w2	
		
	MYMLA_BT\arch	w0, w1, Cx, w2, w4

	add	w1, w2, w2, lsl #2 @uvb:		up*255
	sub	w2, w0, w2 @uvr_helf:	vp*102
		
	@$v5 implementation	
	@smlabb	w4, w6, Cx, xBx				;yp = yp * C2 + xBx;		
	@$v4 implementation	
	@mov		w4, Cx, lsl #16	
	@mov		w4, w4, lsr #16	
	@mla		w4, w6, w4, xBx				;yp = yp * C2 + xBx;	

	MYMLA_BB\arch	w4, w6, Cx, xBx
		
	add	w6, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w6, w6, asr #15 @red5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31
	mov	w3, w6 @rrrrr
		
	add	w6, w4, w1 @blue5  = yp + up*255
	mov	w6, w6, asr #15 @blue5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31
	orr	w3, w6, w3, lsl #11 @rrrrr,??????,bbbbb
		
	ldrb	w6, [y0, #1]! @yp 
	mov	w5, #63
	sub	w4, w4, w0 @green6 = yp - 51*up - 102*vp
	mov	w4, w4, asr #14 @green6
	cmp	w4, w5
	bichi	w4, w5, w4,ASR #31 
	orr	w3, w3, w4, lsl #5 @rrrrr,gggggg,bbbbb

	@2nd	
	@smlabb	w4, w6, Cx, xBx				;yp = yp * C2 + xBx;	
	MYMLA_BB\arch	w4, w6, Cx, xBx
		
	sub	w6, w4, w0 @green6 = yp - 51*up - 102*vp
	mov	w6, w6, asr #14 @green6
	cmp	w6, w5
	bichi	w6, w5, w6,ASR #31
	orr	w3, w3, w6, lsl #21 @?????,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	mov	w5, #31
	add	w6, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w6, w6, asr #15 @red5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31 
	orr	w3, w3, w6, lsl #27 @rrrrr,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	ldrb	w6, [y1, #1]! @yp 
	add	w4, w4, w1 @blue5  = yp + up*255
	mov	w4, w4, asr #15 @blue5
	cmp	w4, w5
	bichi	w4, w5, w4, asr #31
	orr	w3, w3, w4, lsl #16 @rrrrr,gggggg,bbbbb,,(rrrrr,gggggg,bbbbb)
		
	str	w3, [dst, #4]!

	@1st	
	@smlabb	w4, w6, Cx, xBx				;yp = yp * C2 + xBx;	
	MYMLA_BB\arch	w4, w6, Cx, xBx
	add	w6, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w6, w6, asr #15 @red5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31
	mov	w3, w6 @rrrrr
		
	add	w6, w4, w1 @blue5  = yp + up*255
	mov	w6, w6, asr #15 @blue5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31
	orr	w3, w6, w3, lsl #11 @rrrrr,??????,bbbbb
		
	ldrb	w6, [y1, #1]! @yp 
	mov	w5, #63
	sub	w4, w4, w0 @green6 = yp - 51*up - 102*vp
	mov	w4, w4, asr #14 @green6
	cmp	w4, w5
	bichi	w4, w5, w4,ASR #31 
	orr	w3, w3, w4, lsl #5 @rrrrr,gggggg,bbbbb
		
	@2nd	
	@smlabb	w4, w6, Cx, xBx				;yp = yp * C2 + xBx;	
	MYMLA_BB\arch	w4, w6, Cx, xBx
	sub	w6, w4, w0 @green6 = yp - 51*up - 102*vp
	ldr	w0, [sp, #u0_end_SHIFT_u]
	mov	w6, w6, asr #14 @green6
	cmp	w6, w5
	bichi	w6, w5, w6,ASR #31
	orr	w3, w3, w6, lsl #21 @?????,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	mov	w5, #31
	add	w6, w4, w2, lsl #1 @red5   = yp + vp*204
	ldr	w2, [sp, #drb_SHIFT_u]
	mov	w6, w6, asr #15 @red5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31 
	orr	w3, w3, w6, lsl #27 @rrrrr,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	add	w4, w4, w1 @blue5  = yp + up*255
	mov	w4, w4, asr #15 @blue5
	cmp	w4, w5
	bichi	w4, w5, w4, asr #31
	orr	w3, w3, w4, lsl #16 @rrrrr,gggggg,bbbbb,,(rrrrr,gggggg,bbbbb)
		
	str	w3, [dst, w2]

	cmp	u0,w0
	bne	\version\().loop

	add	w6,sp,#height_SHIFT_u @
	ldmia	w6,{height-d_stride} @
	@ldr		yrb,	[sp, #yrb_SHIFT_u]	
	@ldr		uvrb,	[sp, #uvrb_SHIFT_u]	
	@ldr		width,  [sp, #width_SHIFT_u]	
	@ldr		height, [sp, #height_SHIFT_u]	
	@ldr		d_stride, [sp, #d_stride_SHIFT_u]	
		
	add	y0, y0, yrb
	add	y1, y1, yrb
	add	u0, u0, uvrb
	add	w6, u0, width
	str	w6, [sp, #u0_end_SHIFT_u]
	subs	height,height,#1
	str	height,[sp, #height_SHIFT_u]
	add	v0, v0, uvrb
	add	dst, dst, d_stride

	bne	\version\().loop

	add	sp,sp,#CACHE_SHIFT_u
	ldmfd	sp!,{r4-r11,pc}

	.endm


my_FskCopyYUV42016RGB565SE_unity_bc_prototype_arm_v5:

	UNITY_BC_CONVERSION	_v5, unity_bc_v5_
	@ENDP


my_FskCopyYUV42016RGB565SE_unity_bc_prototype_arm_v4:

	UNITY_BC_CONVERSION	_v4, unity_bc_v4_
	@ENDP


@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	UNITY_BC_DUB_VER_CONVERSION arch, version
@;;;;;;;;;;;;;;;;;;;;;;;;;;;

	stmfd	sp!,{r4-r11,lr}
	sub	sp,sp,#CACHE_SHIFT_u
		
	add	w6,sp,#Bx_SHIFT_u @
	ldmia	w6,{xBx-d_stride} 
		
	mov	y1, d_stride
	@str		w6, [sp, #drb_SHIFT_u]		;cache drb here	
		
	mov	yrb, yrb, lsl #1
	mov	d_stride, d_stride, lsl #1
		
	sub	yrb, yrb, width
	sub	uvrb, uvrb, width, lsr #1
	sub	d_stride, d_stride, width, lsl #1

	mov	width, width, lsr #1
	mov	height, height, lsr #1

	add	w6,sp,#height_SHIFT_u @
	stmia	w6,{height-d_stride}

	add	w6, yrb, #1
	mov	w6, #1
	sub	y0, y0, w6
		
	add	w6, uvrb, #1
	mov	w6, #1
	sub	u0, u0, w6
	sub	v0, v0, w6

	add	w6, d_stride, #4
	mov	w6, #4
	sub	dst, dst, w6

	add	w6, u0, width
	str	w6, [sp, #u0_end_SHIFT_u]

\version\().loop:	

	ldrb	w0, [u0, #1]! @u
	ldrb	w1, [v0, #1]! @v
	ldrb	w6, [y0, #1]! @yp 
		
	mov	w5, #31 
	mov	w2, Cx, lsr #17 @51
	sub	w0, w0, #128 @up-128
	mul	w2, w0, w2 @up*51
	sub	w1, w1, #128 @vp-128   
		
	@v5 implementation	
	@smlabt	w0, w1, Cx, w2				;uvg:		vp*102 + up*51	
	@v4 implementation	
	@mov		w4, Cx, lsr #16	
	@mla		w0, w1, w4, w2	
		
	MYMLA_BT\arch	w0, w1, Cx, w2, w4

	add	w1, w2, w2, lsl #2 @uvb:		up*255
	sub	w2, w0, w2 @uvr_helf:	vp*102
		
	@$v5 implementation	
	@smlabb	w4, w6, Cx, xBx				;yp = yp * C2 + xBx;		
	@$v4 implementation	
	@mov		w4, Cx, lsl #16	
	@mov		w4, w4, lsr #16	
	@mla		w4, w6, w4, xBx			;yp = yp * C2 + xBx;	

	MYMLA_BB\arch	w4, w6, Cx, xBx
		
	add	w6, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w6, w6, asr #15 @red5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31
	mov	w3, w6 @rrrrr
		
	add	w6, w4, w1 @blue5  = yp + up*255
	mov	w6, w6, asr #15 @blue5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31
	orr	w3, w6, w3, lsl #11 @rrrrr,??????,bbbbb
		
	ldrb	w6, [y0, #1]! @yp 
	mov	w5, #63
	sub	w4, w4, w0 @green6 = yp - 51*up - 102*vp
	mov	w4, w4, asr #14 @green6
	cmp	w4, w5
	bichi	w4, w5, w4,ASR #31 
	orr	w3, w3, w4, lsl #5 @rrrrr,gggggg,bbbbb

	@2nd	
	@smlabb	w4, w6, Cx, xBx			;yp = yp * C2 + xBx;	
	MYMLA_BB\arch	w4, w6, Cx, xBx
		
	sub	w6, w4, w0 @green6 = yp - 51*up - 102*vp
	ldr	w0, [sp, #u0_end_SHIFT_u]
	mov	w6, w6, asr #14 @green6
	cmp	w6, w5
	bichi	w6, w5, w6,ASR #31
	orr	w3, w3, w6, lsl #21 @?????,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	mov	w5, #31
	add	w6, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w6, w6, asr #15 @red5
	cmp	w6, w5
	bichi	w6, w5, w6, asr #31 
	orr	w3, w3, w6, lsl #27 @rrrrr,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	add	w4, w4, w1 @blue5  = yp + up*255
	mov	w4, w4, asr #15 @blue5
	cmp	w4, w5
	bichi	w4, w5, w4, asr #31
	orr	w3, w3, w4, lsl #16 @rrrrr,gggggg,bbbbb,,(rrrrr,gggggg,bbbbb)
		
	str	w3, [dst, #4]!
	str	w3, [dst, y1]

	cmp	u0,w0
	bne	\version\().loop

	add	w6,sp,#height_SHIFT_u
	ldmia	w6,{height-d_stride}
		
	add	y0, y0, yrb
	add	u0, u0, uvrb
	add	w6, u0, width
	str	w6, [sp, #u0_end_SHIFT_u]
	subs	height,height,#1
	str	height,[sp, #height_SHIFT_u]
	add	v0, v0, uvrb
	add	dst, dst, d_stride

	bne	\version\().loop

	add	sp,sp,#CACHE_SHIFT_u
	ldmfd	sp!,{r4-r11,pc}

	.endm

my_FskCopyYUV42016RGB565SE_unity_bc_dub_ver_prototype_arm_v5:

	UNITY_BC_DUB_VER_CONVERSION	_v5, unity_bc_dub_ver_v5_
	@ENDP

my_FskCopyYUV42016RGB565SE_unity_bc_dub_ver_prototype_arm_v4:

	UNITY_BC_DUB_VER_CONVERSION	_v4, unity_bc_dub_ver_v4_
	@ENDP


		
B1	.req	r5 
CC	.req	r4 
		
@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	CORE_SCALE_BC_v5 version
@;;;;;;;;;;;;;;;;;;;;;;;;;;;

	ldr	B1, [sp, #B1_SHIFT] @B1: ((b88 - (16 * c88)) * 149)
	ldr	CC, [sp, #C_SHIFT] @C1: c88

	mov	r7, r2 @V
	ldr	r12, [sp, #0x80] @xd

\version\().loop_cc_4:	

	@sctatching registers: r0, r1, r2, r3	
	ldrb	r1, [r8, +r9, asr #16] @yp  t=(x >> 16)
	ldrb	r2, [r6, +r9, asr #17] @up
	ldrb	r0, [r7, +r9, asr #17] @vp
		
	smlabb	r11, r1, CC, B1 @yp = yp * C2 + B1;
	sub	r2, r2, #128 @up-128
	sub	r0, r0, #128 @vp-128
		
	mov	r3, CC, lsr #17 @51
	mul	r1, r2, r3 @up*51
	smlabt	r3, r0, CC, r1 @vp*102 + up*51
	add	r2, r1, r1, lsl #2 @up*255
	sub	r1, r3, r1 @vp*102
		
	add	r1, r11, r1, lsl #1 @red5   = yp + vp*204
	add	r0, r11, r2 @blue5  = yp + up*255
	sub	r2, r11, r3 @green6 = yp - 51*up - 102*vp
		
	mov	r3, #63
	mov	r2, r2, asr #14 @green6
	cmp	r2,r3
	bichi	r2,r3,r2,ASR #31
		
	mov	r3, #31
	mov	r1, r1, asr #15 @red5
	cmp	r1,r3
	bichi	r1,r3,r1,ASR #31
		
	orr	r2, r2, r1, lsl #6
		
	mov	r1, r0, asr #15 @blue5
	cmp	r1,r3
	bichi	r1,r3,r1,ASR #31

	add	r9, r9, r12 @x+=xd	
	subs	lr, lr, #1 @width--
	orr	r3, r1, r2, lsl #5 @rgb15
	strh	r3, [r10], #2 @*dst=rgb15

	bne	\version\().loop_cc_4 

	.endm

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	CORE_SCALE_v5 version
@;;;;;;;;;;;;;;;;;;;;;;;;;;;

	mov	r5, #149
	ldr	CC, [sp, #C_SHIFT] @C1: c88

	mov	r7, r2 @V
	ldr	r12, [sp, #0x80] @xd

\version\().loop_cc_4:	

	@sctatching registers: r0, r1, r2, r3	
	ldrb	r1, [r8, +r9, asr #16] @yp  t=(x >> 16)
	ldrb	r2, [r6, +r9, asr #17] @up
	ldrb	r0, [r7, +r9, asr #17] @vp
		
	sub	r11,r1, #16 @yp-16
	sub	r2, r2, #128 @up-128
	sub	r0, r0, #128 @vp-128
		
	mul	r11,r5, r11 @yp = yp * 149
	smulbb	r1, r2, CC @up*51
	smlabt	r3, r0, CC, r1 @vp*102 + up*51
	add	r2, r1, r1, lsl #2 @up*255
	sub	r1, r3, r1 @vp*102
		
	add	r1, r11, r1, lsl #1 @red5   = yp + vp*204
	add	r0, r11, r2 @blue5  = yp + up*255
	sub	r2, r11, r3 @green6 = yp - 51*up - 102*vp
		
	mov	r3, #63
	mov	r2, r2, asr #9 @green6
	cmp	r2,r3
	bichi	r2,r3,r2,ASR #31
		
	mov	r3, #31
	mov	r1, r1, asr #10 @red5
	cmp	r1,r3
	bichi	r1,r3,r1,ASR #31
		
	orr	r2, r2, r1, lsl #6
		
	mov	r1, r0, asr #10 @blue5
	cmp	r1,r3
	bichi	r1,r3,r1,ASR #31

	add	r9, r9, r12 @x+=xd	
	subs	lr, lr, #1 @width--
	orr	r3, r1, r2, lsl #5 @rgb15
	strh	r3, [r10], #2 @*dst=rgb15

	bne	\version\().loop_cc_4 

	.endm

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	CORE_SCALE_BC_v4 version
@;;;;;;;;;;;;;;;;;;;;;;;;;;;

	ldr	B1, [sp, #B1_SHIFT] @B1: ((b88 - (16 * c88)) * 149)
	ldr	CC, [sp, #C_SHIFT] @C1: c88

	mov	r7, r2 @V
	ldr	r12, [sp, #0x80] @xd

\version\().loop_cc_4:	

	ldrb	r1, [r8, +r9, asr #16] @yp  t=(x >> 16)
	ldrb	r2, [r6, +r9, asr #17] @up
	ldrb	r0, [r7, +r9, asr #17] @vp
		
	mov	r3, CC, lsl #16
	mov	r3, r3, lsr #16
	mla	r11, r1, r3, B1 @yp = yp * C2 + B1;
	sub	r2, r2, #128 @up-128
	sub	r0, r0, #128 @vp-128
		
	mov	r3, CC, lsr #17 @51
	mul	r1, r2, r3 @up*51
	mov	r3, CC, lsr #16 @51
	mla	r3, r0, r3, r1 @vp*102 + up*51
	add	r2, r1, r1, lsl #2 @up*255
	sub	r1, r3, r1 @vp*102
		
	add	r1, r11, r1, lsl #1 @red5   = yp + vp*204
	add	r0, r11, r2 @blue5  = yp + up*255
	sub	r2, r11, r3 @green6 = yp - 51*up - 102*vp
		
	mov	r3, #63
	mov	r2, r2, asr #14 @green6
	cmp	r2,r3
	bichi	r2,r3,r2,ASR #31
		
	mov	r3, #31
	mov	r1, r1, asr #15 @red5
	cmp	r1,r3
	bichi	r1,r3,r1,ASR #31
		
	orr	r2, r2, r1, lsl #6
		
	mov	r1, r0, asr #15 @blue5
	cmp	r1,r3
	bichi	r1,r3,r1,ASR #31

	add	r9, r9, r12 @x+=xd	
	subs	lr, lr, #1 @width--
	orr	r3, r1, r2, lsl #5 @rgb15
	strh	r3, [r10], #2 @*dst=rgb15

	bne	\version\().loop_cc_4 

	.endm

	.equ NO_REGIS, (14)
	.equ NO_CACHE, (11)
	.equ SP_SHIFT, (NO_CACHE*4 + NO_REGIS*4 )


	.equ C2_SHIFT, (52+SP_SHIFT)
	.equ C1_SHIFT, (48+SP_SHIFT)
	.equ C_SHIFT, (48+SP_SHIFT)
	.equ B1_SHIFT, (44+SP_SHIFT)
	.equ yCmax_SHIFT, (40+SP_SHIFT)
	.equ xCmax_SHIFT, (36+SP_SHIFT)
	.equ yd_SHIFT, (32+SP_SHIFT)
	.equ xd_SHIFT, (28+SP_SHIFT)
	.equ y0_SHIFT, (24+SP_SHIFT)
	.equ x0_SHIFT, (20+SP_SHIFT)
	.equ d0_SHIFT, (16+SP_SHIFT)
	.equ V0_SHIFT, (12+SP_SHIFT)
	.equ U0_SHIFT, (8 +SP_SHIFT)
	.equ Y0_SHIFT, (4 +SP_SHIFT)
	.equ drb_SHIFT, (0 +SP_SHIFT)

	.equ Crb_SHIFT, (14*4)
	.equ Yrb_SHIFT, (13*4)
	.equ width_SHIFT, (11*4)
	.equ height_SHIFT, (10*4)
	.equ last_ty_SHIFT, (9*4)
	.equ y_SHIFT, (7*4)
	.equ Yr_SHIFT, (6*4)
	.equ V_SHIFT, (5*4)
	.equ U_SHIFT, (4*4)
	.equ m3_SHIFT, (3*4)
	.equ m2_SHIFT, (2*4)
	.equ m1_SHIFT, (1*4)

@Crb 	=> r3
@Yrb 	=> r2
@height => r1
@width 	=> r0	

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	COLOR_CONVERSION_SCALE arch, version
@;;;;;;;;;;;;;;;;;;;;;;;;;;; 

	stmdb	sp!, {r0 - r12, lr}
	sub	sp, sp, #0x2C
	mov	lr, r0 @(lr)==>width
	ldr	r5, [sp, #d0_SHIFT] @d0
	ldr	r10,[sp, #yd_SHIFT] @yd
	ldr	r7, [sp, #y0_SHIFT] @y0
	ldr	r4, [sp, #drb_SHIFT] @drb
	mov	r9, r2 @Yrb
	mov	r0, r3 @Crb
	movs	r11, r1 @(i=height)==0?
	beq	\version\().loop_cc_1 

	mov	r9, r4, asr #1 @drb>>1
	sub	r6, r5, r9, lsl #1 @(d0-(drb>>1))
	str	r6, [sp, #4] @(d0-(drb>>1))
	str	r9, [sp, #0x08] @drb>>1
	sub	r8, r7, #2, 18 @y0-(1L << 15)
	mvn	r1, #0

\version\().loop_cc_5:	
	mov	r4, r7, asr #16 @t=(y >> 16)
	sub	r11,r11, #1 @i--
	cmp	r4, r1 @t==0?
	str	r11, [sp, #0x28] @i
	bne	\version\().loop_cc_2 
	mov	r2, lr, lsl #1 @width<<1
	mov	r1, r6 @(d0-(drb>>1))
	mov	r0, r5 @d0
	bl	memcpy @memcpy( (void *)d0, (void *)(d0-(drb>>1)), width<<1 )
	mov	r1, r4 @t	
	add	r7, r7, r10 @y += yd
	add	r8, r8, r10 @y0-(1L << 15)  += yd
	b	\version\().loop_cc_3 

\version\().loop_cc_2:	
	ldr	r3, [sp, #0x34] @Yrb
	ldr	r2, [sp, #Y0_SHIFT] @Y0
	mov	r1, r4 @last_ty = t
	add	r7, r7, r10 @y += yd
	mla	r3, r4, r3, r2 @Yr = t*Yrb + Y0
	ldr	r4, [sp, #V0_SHIFT] @V0
	str	r1, [sp, #0x24] @(S24)==>last_ty
	str	r3, [sp, #0x18] @(S18)==>Yr
	movs	r3, r8, asr #1 @yC=(y - (1L << 15)) >> 1
	ldrpl	r2, [sp, #yCmax_SHIFT] @yCmax
	movmi	r3, #0 @yC = 0
	add	r8, r8, r10 @(y - (1L << 15)) += yd
	cmppl	r3, r2 @yC>yCmax?
	movgt	r3, r2 @yC = yCmax;
	ldr	r2, [sp, #U0_SHIFT] @U0
	mov	r3, r3, asr #16 @t = (yC >> 16)
	str	r7, [sp, #0x1C] @(S1C)==>y
	mla	r2, r3, r0, r2 @U0 += t * Crb
	str	r8, [sp, #0x20] @
	@str         r5, [sp, #0x08]			;???	
	str	r2, [sp, #0x10] @
	mla	r2, r3, r0, r4
	cmp	lr, #0
	str	r2, [sp, #0x14] @V
	mov	r3, lr
	str	r3, [sp, #0x0C] @

	beq	\version\().loop_cc_3 
	ldr	r9, [sp, #x0_SHIFT] @x=x0
	ldr	r6, [sp, #0x10] @U
	ldr	r8, [sp, #0x18] @Y
	mov	r10, r5 @dst = d0

	@;;;;;;;;;;;;;;;;;;;;;;;;;;;;	
		
	CORE_SCALE_BC\arch	\version
		
	@;;;;;;;;;;;;;;;;;;;;;;;;;;;;	
		
	ldr	r5, [sp, #d0_SHIFT] @d0
	ldr	r6, [sp, #4] @
	ldr	r7, [sp, #0x1C] @
	ldr	r8, [sp, #0x20] @
	ldr	r9, [sp, #0x08] @drb>>1
	ldr	r10,[sp, #yd_SHIFT] @yd
	ldr	r11,[sp, #0x28] @height
	ldr	r1, [sp, #0x24] @

\version\().loop_cc_3:	
	add	r6, r6, r9, lsl #1
	add	r5, r5, r9, lsl #1
	ldr	lr, [sp, #0x2C] @width
	ldr	r0, [sp, #0x38] @Crb
	str	r6, [sp, #4] @
	str	r5, [sp, #d0_SHIFT] @d0
	cmp	r11, #0
	bne	\version\().loop_cc_5 

\version\().loop_cc_1:	

	add	sp, sp, #0x2C
	ldmia	sp!, {r0 - r12, pc}

	.endm


my_FskCopyYUV42016RGB565SE_scale_bc_prototype_arm_v5:
	COLOR_CONVERSION_SCALE	_v5, scale_bc_v5
	@ENDP

my_FskCopyYUV42016RGB565SE_scale_bc_prototype_arm_v4:
	COLOR_CONVERSION_SCALE	_v4, scale_bc_v4
	@ENDP

@|my_FskCopyYUV42016RGB565SE_scale_prototype_arm_v5| PROC
@	COLOR_CONVERSION_SCALE _v5, scale_v5
@  ENDP

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	GET_PATTERN switch, mark
@;;;;;;;;;;;;;;;;;;;;;;;;;;;
	ldrb	w6, [pattern, #1]!
	add	w5, pc, #(\switch - \mark)
	ldr	pc, [w5, w6, lsl #2]

\mark:	
	.endm

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	COLOR_CONVERSION_UNITY version
@;;;;;;;;;;;;;;;;;;;;;;;;;;; 

	.unreq	width
width	.req	r0 
	.unreq	height
height	.req	r1 

	.unreq	dst
dst	.req	r5 
dst_next	.req	r0 
sy	.req	r7 


CONST_r14	.req	r14 
CNST_65535	.req	r1 

	.equ NUM_REGIS_un, (13)
	.equ NUM_CACHE_un, (1)
	.equ SP_SHIFT_un, (NUM_REGIS_un*4 + NUM_CACHE_un*4 )


	.equ width_half_SHIFT_un, (4)
	.equ height_half_SHIFT_un, (8)
	.equ sy_SHIFT_un, (12)
	.equ srb_SHIFT_un, (16)

	.equ su_SHIFT_un, ( 0+SP_SHIFT_un)
	.equ sv_SHIFT_un, ( 4+SP_SHIFT_un)
	.equ srb_half_SHIFT_un, ( 8+SP_SHIFT_un)
	.equ dst_SHIFT_un, (12+SP_SHIFT_un)
	.equ drb_SHIFT_un, (16+SP_SHIFT_un)


	stmfd	sp!,{r0-r11,lr}
	sub	sp,sp,#4
	movs	r0,height,LSR #1
	str	r0,[sp,#8]
	ldr	r0,[sp,#4]
	movs	r0,r0,LSR #1
	str	r0,[sp,#4]
		
	mov	CNST_65535,#0x10000 @CNST_65535
	sub	CNST_65535,CNST_65535,#1
		
	mov	CONST_r14,#0x2a
	add	CONST_r14,CONST_r14,#0x100 @CONST_r14

\version\().loop_2:	
	ldr	r3,[sp,#srb_SHIFT_un] @=>srb
	ldr	sy,[sp,#sy_SHIFT_un] @=>sy
	ldr	r0,[sp,#srb_half_SHIFT_un] @=>srb_half
	add	r6,sp,#su_SHIFT_un
	ldmia	r6,{r4,r6} @=>su, sv
		
	add	r9,sp,#dst_SHIFT_un @
	ldmia	r9,{dst,r9} @=>dst, drb
		
	add	r8,sy,r3 @
	add	r2,r8,r3 @
	add	r3,r4,r0 @
	add	r12,r6,r0 @
	str	r2,[sp,#sy_SHIFT_un] @<=sy
	ldr	r10,[sp,#width_half_SHIFT_un] @=>width_half
		
	add	r2,sp,#su_SHIFT_un 
	stmia	r2,{r3,r12} @<=su, sv
	add	dst_next,dst,r9
	add	r9,dst_next,r9
	add	r10,r4,r10
	str	r10,[sp,#0]
	str	r9,[sp,#dst_SHIFT_un]

\version\().loop_1:	
	ldrb	r12,[sy],#1
	ldrb	r2, [r6],#1
	ldrb	r3, [r4],#1
	mul	r9, CONST_r14,r12
	rsb	r10, r2,r2,LSL #6
	rsb	r10,r10,r10,LSL #3
	sub	r10,r10,r2,LSL #5
	add	r12,r2,r2,LSL #2
	add	r2,r12,r2,LSL #3
	add	r12,r3,r3,LSL #3
	add	r12,r12,r3,LSL #4
	mov	r12,r12,LSL #2
	add	r2,r12,r2,LSL #4
	add	r12,r3,r3,LSL #2
	add	r12,r12,r3,LSL #9
	sub	r3, r10,#0xd000
	sub	r3, r3,#0xb20
	add	r10,r9,r3
	cmp	r10,CNST_65535
	bichi	r10,CNST_65535,r10,ASR #31
		
	rsb	r2,r2,#0x960
	add	r2,r2,#0x8000
	and	r11,r10,#0xf800
	add	r10,r9,r2
	cmp	r10,CNST_65535
	bichi	r10,CNST_65535,r10,ASR #31
		
	mov	r10,r10,ASR #10
	sub	r12,r12,#0x11000
	sub	r12,r12,#0x120
	orr	r11,r11,r10,LSL #5
	add	r10,r9,r12
	ldrb	r9,[sy],#1
	cmp	r10,CNST_65535
	bichi	r10,CNST_65535,r10,ASR #31
		
	orr	r11,r11,r10,ASR #11
	mul	r9,CONST_r14,r9
	add	r10,r9,r2
	cmp	r10,CNST_65535
	bichi	r10,CNST_65535,r10,ASR #31
		
	mov	r10,r10,ASR #10
	orr	r11,r11,r10,LSL #21
	add	r10,r9,r3
	cmp	r10,CNST_65535
	bichi	r10,CNST_65535,r10,ASR #31
		
	add	r9,r9,r12
	cmp	r9,CNST_65535
	bichi	r9,CNST_65535,r9,ASR #31
		
	and	r10,r10,#0xf800
	orr	r9,r10,r9,ASR #11
	ldrb	r10,[r8],#1
	orr	r9,r11,r9,LSL #16
	str	r9,[r5],#4
	mul	r10,CONST_r14,r10
	add	r9,r10,r3
	cmp	r9,CNST_65535
	bichi	r9,CNST_65535,r9,ASR #31
		
	and	r11,r9,#0xf800
	add	r9,r10,r2
	cmp	r9,CNST_65535
	bichi	r9,CNST_65535,r9,ASR #31
		
	mov	r9,r9,ASR #10
	orr	r9,r11,r9,LSL #5
	ldrb	r11,[r8],#1
	add	r10,r10,r12
	cmp	r10,CNST_65535
	mul	r11,CONST_r14,r11
	bichi	r10,CNST_65535,r10,ASR #31
		
	add	r2,r11,r2
	cmp	r2,CNST_65535
	bichi	r2,CNST_65535,r2,ASR #31
		
	add	r3,r11,r3
	cmp	r3,CNST_65535
	bichi	r3,CNST_65535,r3,ASR #31
		
	add	r12,r11,r12
	cmp	r12,CNST_65535
	bichi	r12,CNST_65535,r12,ASR #31
		
	and	r3,r3,#0xf800
	orr	r3,r3,r12,ASR #11
	ldr	r12,[sp,#0]
	mov	r2,r2,ASR #10
	orr	r9,r9,r10,ASR #11
	orr	r2,r9,r2,LSL #21
	orr	r2,r2,r3,LSL #16
	str	r2,[dst_next],#4
	cmp	r4,r12
	bne	\version\().loop_1

	ldr	r0,[sp,#8]
	subs	r0,r0,#1
	str	r0,[sp,#8]
	bne	\version\().loop_2

	add	sp,sp,#20
	ldmfd	sp!,{r4-r11,pc}

	.endm

my_FskCopyYUV42016RGB565SE_unity_prototype_arm_v4:
	COLOR_CONVERSION_UNITY	unity_v4
	@ENDP

		
my_FskCopyYUV42016RGB565SE_scale_prototype_arm_v4:
		
	stmdb	sp!, {r4 - r12, lr}
	sub	sp, sp, #0x5C
	ldr	r4, [r0] @Y0		???
	ldr	r3, [r0, #0xC] @
	ldr	r1, [r0, #4] @Yrb
	str	r4, [sp, #0xC] @
	ldr	r4, [r0, #0x30] @U0
	mov	r3, r3, lsr #1 @
	mov	r3, r3, lsl #16 @
	str	r4, [sp, #0x14] @
	ldr	r4, [r0, #0x34] @V0
	ldr	r2, [r0, #0x1C] @dstHeight
	sub	r3, r3, #1, 16 @
	str	r4, [sp, #0x18] @
	ldr	r4, [r0, #0x20] @
	ldr	lr, [r0, #0x18] @dstWidth			;
	ldr	r5, [r0, #0x10] @d0
	ldr	r10, [r0, #0x2C] @
	str	r4, [sp, #0x1C] @
	ldr	r4, [r0, #0x28] @
	ldr	r7, [r0, #0x24] @
	str	r3, [sp, #0x10] @
	ldr	r3, [r0, #0x14] @drb
	mov	r0, r1, asr #1
	str	r1, [sp, #8]
	str	r4, [sp, #0x38]
	@mov			r12, r4	
	str	lr, [sp, #0x54]
	str	r5, [sp]
	str	r10, [sp, #0x48]
	str	r0, [sp, #0x58]
	mvn	r1, #0
	movs	r11, r2
	beq	loop_81_1 @|my_FskCopyYUV42016RGB565SE_81 + 0x230 ( 11474h )| 
	mov	r9, r3, asr #1
	sub	r6, r5, r9, lsl #1
	str	r6, [sp, #4]
	str	r9, [sp, #0x44]
	sub	r8, r7, #2, 18
loop_81_5:	
	mov	r4, r7, asr #16
	sub	r11, r11, #1
	cmp	r4, r1
	str	r11, [sp, #0x4C]
	bne	loop_81_2 @|my_FskCopyYUV42016RGB565SE_81 + 0xcc ( 11310h )| 
	mov	r2, lr, lsl #1
	mov	r1, r6
	mov	r0, r5
	bl	memcpy
	mov	r1, r4
	add	r7, r7, r10
	add	r8, r8, r10
	b	loop_81_3 @|my_FskCopyYUV42016RGB565SE_81 + 0x210 ( 11454h )| 

loop_81_2:	
	ldr	r3, [sp, #8]
	ldr	r2, [sp, #0xC]
	mov	r1, r4
	add	r7, r7, r10
	mla	r3, r4, r3, r2
	ldr	r4, [sp, #0x18]
	str	r1, [sp, #0x50]
	str	r3, [sp, #0x34]
	movs	r3, r8, asr #1
	ldrpl	r2, [sp, #0x10]
	movmi	r3, #0
	add	r8, r8, r10
	cmppl	r3, r2
	movgt	r3, r2
	ldr	r2, [sp, #0x14]
	mov	r3, r3, asr #16
	str	r7, [sp, #0x3C]
	mla	r2, r3, r0, r2
	str	r8, [sp, #0x40]
	str	r5, [sp, #0x24]
	str	r2, [sp, #0x2C]
	mla	r2, r3, r0, r4
	ldr	r3, [sp, #0x1C]
	cmp	lr, #0
	str	r2, [sp, #0x30]
	str	r3, [sp, #0x20]
	mov	r3, lr
	str	r3, [sp, #0x28]

	beq	loop_81_3 @ |my_FskCopyYUV42016RGB565SE_81 + 0x210 ( 11454h )| 
	ldr	r9, [sp, #0x20]
	ldr	r6, [sp, #0x2C]
	ldr	r8, [sp, #0x34]
	mov	r10, r5
	ldr	r5, copy_pin_mark_81_1 @;offset to from next line to pin_31
	add	r5,pc,r5 @;pin_31

	mov	r11, lr
copy_pin_mark_81_0:	

	mov	r7, r2
	ldr	r12, [sp, #0x38]

loop_81_4:	

	@yp = 149*(*Y);	
	@up = *(U);	
	@vp = *(V);	

	@r = (yp +            204 * vp - 28016)>>10;	
	@g = (yp - 50  * up - 104 * vp + 17600)>>9;	
	@b = (yp + 258 * up			   - 34928)>>10;				

	ldrb	r2, [r8, +r9, asr #16] @yp  dx
	ldrb	r4, [r6, +r9, asr #17] @up
	ldrb	r0, [r7, +r9, asr #17] @vp
	mov	r1, #0x95 @149
	mul	lr, r2, r1 @yp = (*Y)*149
	mov	r2, #0x32 @50

	rsb	r3, r4, #0x16, 28 @X - up
	mul	r2, r3, r2 @(X - up)*50
	mov	r3, #0x68 @104
	mul	r3, r0, r3 @vp * 104
	mov	r1, #0xCC @204
	mla	r1, r0, r1, lr @yp + vp * 104
	sub	r3, r2, r3 @(X - up)*50 - vp * 104
		
	add	r2, r4, r4, lsl #8 @up*257  approximation of 258
	add	r0, r2, lr @yp + up*257
		
	add	r3, r3, lr @g6 = yp + (X - up)*50 - vp * 104
	sub	r4, r1, #0x6D, 24 @r5 = yp + vp * 104 - 27904 approximation of 28016
		
	add	r1, r5, r3, asr #9 @g6
		
	ldrb	r2, [r1, #0x9C] @g6
	add	lr, r5, r4, asr #10 @r5
	ldrb	r4, [lr, #0x24] @b5
	sub	r3, r0, #0x22, 22 @b5 = yp + up*257 - 34816  approximation of 34928
		
	add	r0, r5, r3, asr #10 @b5
	orr	r2, r2, r4, lsl #6
	ldrb	r3, [r0, #0x24] @b5

	add	r9, r9, r12 
	subs	r11, r11, #1

	orr	r3, r3, r2, lsl #5

	strh	r3, [r10], #2 
		

	bne	loop_81_4 @|my_FskCopyYUV42016RGB565SE_81 + 0x15c ( 113a0h )|  
		
	@;;;;;;;;;;;;;;;;;;;;;;;;;;;;	
		
	ldr	r5, [sp]
	ldr	r6, [sp, #4]
	ldr	r7, [sp, #0x3C]
	ldr	r8, [sp, #0x40]
	ldr	r9, [sp, #0x44]
	ldr	r10, [sp, #0x48]
	ldr	r11, [sp, #0x4C]
	ldr	r1, [sp, #0x50]

loop_81_3:	
	add	r6, r6, r9, lsl #1
	add	r5, r5, r9, lsl #1
	ldr	lr, [sp, #0x54]
	ldr	r0, [sp, #0x58]
	str	r6, [sp, #4]
	str	r5, [sp]
	cmp	r11, #0
	bne	loop_81_5 @|my_FskCopyYUV42016RGB565SE_81 + 0x98 ( 112dch )| 

loop_81_1:	

	add	sp, sp, #0x5C
	ldmia	sp!, {r4 - r12, pc}
	andeq	r4, r1, r8, ror r0

copy_pin_mark_81_1:	
	.long	copy_pin_mark_81_3 - copy_pin_mark_81_0
copy_pin_mark_81_2:	
	.long	copy_pin_mark_81_4 - copy_pin_mark_81_0
copy_pin_mark_81_3:	
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x03020100
	.long	0x07060504
	.long	0x0b0a0908
	.long	0x0f0e0d0c
	.long	0x13121110
	.long	0x17161514
	.long	0x1b1a1918
	.long	0x1f1e1d1c
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
	.long	0x1f1f1f1f
copy_pin_mark_81_4:	
	@DCD		0x00000000	
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x00000000
	.long	0x03020100
	.long	0x07060504
	.long	0x0b0a0908
	.long	0x0f0e0d0c
	.long	0x13121110
	.long	0x17161514
	.long	0x1b1a1918
	.long	0x1f1e1d1c
	.long	0x23222120
	.long	0x27262524
	.long	0x2b2a2928
	.long	0x2f2e2d2c
	.long	0x33323130
	.long	0x37363534
	.long	0x3b3a3938
	.long	0x3f3e3d3c
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f
	.long	0x3f3f3f3f

	@ENDP




@
@
@ interlace case
@
@

@scale factor		1.0  0.98 0.90 0.80 0.75 0.60 0.50 0.40 0.33
@analog				 52    55   64   80   90  136  187  283  400
@pattern			 65    69   75   88   95  ???  177  233  306
@pattern interlaced  74    75   86  101  107  142  174  237  306
@unity   interlaced  76   xx
@unity				 72   xx

@scale factor		1.0  1.10 1.20 1.33 1.50 1.75 2.0 3.0 4.0
@analog				 52   47   43  37    33  28   22  13   9
@pattern interlaced  74   71    67  61    52  42   31  xx   xx


yuv	.req	r0 
	.unreq	dst
dst	.req	r1 
	.unreq	xBx
xBx	.req	r2 
	.unreq	Cx
Cx	.req	r3 
line_end	.req	r4 
drb	.req	r5 
CONST_31	.req	r6 
CONST_63	.req	r7 

height_s	.req	r8 
width_s	.req	r9 
yuvrb_s	.req	r10 
drb_s	.req	r11 

	.unreq	w0
w0	.req	r8 
	.unreq	w1
w1	.req	r9 
	.unreq	w2
w2	.req	r10 
	.unreq	w3
w3	.req	r11 
	.unreq	w4
w4	.req	r12 
	.unreq	w5
w5	.req	r14 


	.equ REGIS_SHIFT_i, (9*4)
	.equ CACHE_SHIFT_i, (4*4)
	.equ SP_SHIFT_i, (REGIS_SHIFT_i + CACHE_SHIFT_i)

	.equ yuv_stride_SHIFT_i, (0)
	.equ yuv_width_bytes_SHIFT_i, (4)
	.equ dst_stride_SHIFT_i, (8)
	.equ height_half_SHIFT_i, (12)
	.equ height_SHIFT_i, (0*4 + SP_SHIFT_i)
	.equ width_SHIFT_i, (1*4 + SP_SHIFT_i)
	.equ yuvrb_SHIFT_i, (2*4 + SP_SHIFT_i)
	.equ drb_SHIFT_i, (3*4 + SP_SHIFT_i)

@;;;;;;;;;;;;;;;;;;;;;;;;;;;		

	.macro	UNITY_BC_INTERLACE_CONVERSION arch, version
@;;;;;;;;;;;;;;;;;;;;;;;;;;;

	stmfd	sp!,{r4-r11,lr}
	sub	sp,sp,#CACHE_SHIFT_i
		
	add	w5,sp,#height_SHIFT_i
	ldmia	w5,{height_s-drb_s} 
		
	sub	w4, drb_s, width_s
	mov	w4, w4, asl #1
	str	w4, [sp, #dst_stride_SHIFT_i]
		
	mov	drb, drb_s
		
	add	w4, width_s, width_s, lsl #1 @yuv_width_bytes = width * 3
	str	w4, [sp, #yuv_width_bytes_SHIFT_i]
		
	add	line_end, yuv, w4

	sub	w4, yuvrb_s, w4
	str	w4, [sp, #yuv_stride_SHIFT_i]
		
	mov	height_s, height_s, lsr #1
	str	height_s, [sp, #height_half_SHIFT_i]
	sub	dst, dst, #4
	mov	CONST_31, #31
	mov	CONST_63, #63
		
\version\().loop:	

	ldrb	w0, [yuv], #1 @u
	ldrb	w1, [yuv], #1 @v
	ldrb	w5, [yuv], #1 @yp 
		
	mov	w2, Cx, lsr #17 @51
	sub	w0, w0, #128 @up-128
	mul	w2, w0, w2 @up*51
	sub	w1, w1, #128 @vp-128   
		
	MYMLA_BT\arch	w0, w1, Cx, w2, w4

	add	w1, w2, w2, lsl #2 @uvb:		up*255
	sub	w2, w0, w2 @uvr_helf:	vp*102
		
	MYMLA_BB\arch	w4, w5, Cx, xBx
		
	add	w5, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w5, w5, asr #15 @red5
	cmp	w5, CONST_31
	bichi	w5, CONST_31, w5, asr #31
	mov	w3, w5 @rrrrr
		
	add	w5, w4, w1 @blue5  = yp + up*255
	mov	w5, w5, asr #15 @blue5
	cmp	w5, CONST_31
	bichi	w5, CONST_31, w5, asr #31
	orr	w3, w5, w3, lsl #11 @rrrrr,??????,bbbbb
		
	ldrb	w5, [yuv], #1 @yp 
	sub	w4, w4, w0 @green6 = yp - 51*up - 102*vp
	mov	w4, w4, asr #14 @green6
	cmp	w4, CONST_63
	bichi	w4, CONST_63, w4,ASR #31 
	orr	w3, w3, w4, lsl #5 @rrrrr,gggggg,bbbbb

	@2nd	
	MYMLA_BB\arch	w4, w5, Cx, xBx
		
	sub	w5, w4, w0 @green6 = yp - 51*up - 102*vp
	mov	w5, w5, asr #14 @green6
	cmp	w5, CONST_63
	bichi	w5, CONST_63, w5,ASR #31
	orr	w3, w3, w5, lsl #21 @?????,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	add	w5, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w5, w5, asr #15 @red5
	cmp	w5, CONST_31
	bichi	w5, CONST_31, w5, asr #31 
	orr	w3, w3, w5, lsl #27 @rrrrr,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	ldrb	w5, [yuv], #1 @yp 
	add	w4, w4, w1 @blue5  = yp + up*255
	mov	w4, w4, asr #15 @blue5
	cmp	w4, CONST_31
	bichi	w4, CONST_31, w4, asr #31
	orr	w3, w3, w4, lsl #16 @rrrrr,gggggg,bbbbb,,(rrrrr,gggggg,bbbbb)
		
	str	w3, [dst, #4]!

	@1st	
	MYMLA_BB\arch	w4, w5, Cx, xBx
	add	w5, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w5, w5, asr #15 @red5
	cmp	w5, CONST_31
	bichi	w5, CONST_31, w5, asr #31
	mov	w3, w5 @rrrrr
		
	add	w5, w4, w1 @blue5  = yp + up*255
	mov	w5, w5, asr #15 @blue5
	cmp	w5, CONST_31
	bichi	w5, CONST_31, w5, asr #31
	orr	w3, w5, w3, lsl #11 @rrrrr,??????,bbbbb
		
	ldrb	w5, [yuv], #1 @yp 
	sub	w4, w4, w0 @green6 = yp - 51*up - 102*vp
	mov	w4, w4, asr #14 @green6
	cmp	w4, CONST_63
	bichi	w4, CONST_63, w4,ASR #31 
	orr	w3, w3, w4, lsl #5 @rrrrr,gggggg,bbbbb
		
	@2nd	
	MYMLA_BB\arch	w4, w5, Cx, xBx
	sub	w5, w4, w0 @green6 = yp - 51*up - 102*vp
	mov	w5, w5, asr #14 @green6
	cmp	w5, CONST_63
	bichi	w5, CONST_63, w5,ASR #31
	orr	w3, w3, w5, lsl #21 @?????,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	add	w5, w4, w2, lsl #1 @red5   = yp + vp*204
	mov	w5, w5, asr #15 @red5
	cmp	w5, CONST_31
	bichi	w5, CONST_31, w5, asr #31 
	orr	w3, w3, w5, lsl #27 @rrrrr,gggggg,?????,,(rrrrr,gggggg,bbbbb)
		
	add	w4, w4, w1 @blue5  = yp + up*255
	mov	w4, w4, asr #15 @blue5
	cmp	w4, CONST_31
	bichi	w4, CONST_31, w4, asr #31
	orr	w3, w3, w4, lsl #16 @rrrrr,gggggg,bbbbb,,(rrrrr,gggggg,bbbbb)
		
	str	w3, [dst, drb]

	cmp	yuv,line_end
	bne	\version\().loop

yuv_stride	.req	r8 
yuv_width_bytes	.req	r9 
dst_stride	.req	r10 
height_half	.req	r11 

	add	w5,sp,#yuv_stride_SHIFT_i
	ldmia	w5,{yuv_stride-height_half}
		
	add	yuv, yuv, yuv_stride
	add	dst, dst, dst_stride
	add	line_end, yuv, yuv_width_bytes
	subs	height_half,height_half,#1
	str	height_half,[sp, #height_half_SHIFT_i]

	bne	\version\().loop

	add	sp,sp,#CACHE_SHIFT_i
	ldmfd	sp!,{r4-r11,pc}

	.endm


yuv420ito16RGB565SE_unity_bc_arm_v5:
	UNITY_BC_INTERLACE_CONVERSION	_v5, unity_bc_i_v5_
	@ENDP

yuv420ito16RGB565SE_unity_bc_arm_v4:
	UNITY_BC_INTERLACE_CONVERSION	_v4, unity_bc_i_v4_
	@ENDP


	.end
#endif
