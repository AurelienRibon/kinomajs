#if defined(__linux__) && defined(__ELF__)
.section .note.GNU-stack,"",%progbits /* mark stack as non-executable */
#endif

.text
.fpu neon
.arch armv7a
.object_arch armv7a
.arm

buf_addr	.req r0
len_addr	.req r1
len			.req ip
s1_ptr		.req r2
s2_ptr		.req r3
tmp			.req ip

var_s1		.req r4
var_s2		.req r5
var_f		.req r6
var_n		.req r7
var_k		.req r8
var_j		.req r9
v_nmax		.req r10


@@@@@@NEON Register
vord	.req	q0
vord_a	.req	q1
vs1		.req	q2
vs2		.req	q3
in16	.req	q4
v0		.req	q5
vtmp	.req	q10

vs1_r	.req	q6
vs2_lo	.req	q7
vs2_hi	.req	q8

vdup_k	.req	q6
dst_tmp	.req	q9

v_tsum	.req	d22


.balign 16
adler_vord_const:
	.byte	16
	.byte	15
	.byte	14
	.byte	13
	.byte	12
	.byte	11
	.byte	10
	.byte	9
	.byte	8
	.byte	7
	.byte	6
	.byte	5
	.byte	4
	.byte	3
	.byte	2
	.byte	1

.equ VNMAX, 44416	

.macro	VECTOR_REDUCE	dst
	vshl.i32	dst_tmp, \dst, #16
	vshr.u32	\dst, \dst, #16
	vshr.u32	dst_tmp, dst_tmp, #16
	vsub.i32	dst_tmp, dst_tmp, \dst
	
	vshl.i32	\dst, \dst, #4
	vadd.i32	\dst, dst_tmp, \dst
.endm

.macro	NEON_SIMPLE_ALIGN	dst, a, b, amount
neon_simple_align_start_\a:
	cmp	\amount, #8
	blo	neon_simple_align_0_7_\a
	bhs	neon_simple_align_8_15_\a

neon_simple_align_0_7_\a:
	cmp	\amount, #4
	blo neon_simple_align_0_3_\a
	bhs	neon_simple_align_4_7_\a

neon_simple_align_0_3_\a:
	cmp	\amount, #0
	bne	neon_simple_align_1_\a
	vext.8	\dst, \a, \b, #0
	b	neon_simple_align_end_\a

neon_simple_align_1_\a:
	cmp	\amount, #1
	bne	neon_simple_align_2_\a
	vext.8	\dst, \a, \b, #1
	b	neon_simple_align_end_\a

neon_simple_align_2_\a:
	cmp	\amount, #2
	bne	neon_simple_align_3_\a
	vext.8	\dst, \a, \b, #2
	b	neon_simple_align_end_\a

neon_simple_align_3_\a:
	vext.8	\dst, \a, \b, #3
	b	neon_simple_align_end_\a

neon_simple_align_4_7_\a:
	cmp	\amount, #4
	bne	neon_simple_align_5_\a
	vext.8	\dst, \a, \b, #4
	b	neon_simple_align_end_\a

neon_simple_align_5_\a:
	cmp	\amount, #5
	bne	neon_simple_align_6_\a
	vext.8	\dst, \a, \b, #5
	b	neon_simple_align_end_\a

neon_simple_align_6_\a:
	cmp	\amount, #6
	bne	neon_simple_align_7_\a
	vext.8	\dst, \a, \b, #6
	b	neon_simple_align_end_\a

neon_simple_align_7_\a:
	vext.8	\dst, \a, \b, #7
	b	neon_simple_align_end_\a

neon_simple_align_8_15_\a:
	cmp	\amount, #12
	blo	neon_simple_align_8_11_\a
	bhs	neon_simple_align_12_15_\a

neon_simple_align_8_11_\a:
	cmp \amount, #8
	bne	neon_simple_align_9_\a
	vext.8	\dst, \a, \b, #8
	b	neon_simple_align_end_\a

neon_simple_align_9_\a:
	cmp \amount, #9
	bne	neon_simple_align_10_\a
	vext.8	\dst, \a, \b, #9
	b	neon_simple_align_end_\a

neon_simple_align_10_\a:
	cmp \amount, #10
	bne	neon_simple_align_11_\a
	vext.8	\dst, \a, \b, #10
	b	neon_simple_align_end_\a

neon_simple_align_11_\a:
	vext.8	\dst, \a, \b, #11
	b	neon_simple_align_end_\a

neon_simple_align_12_15_\a:
	cmp	\amount, #12
	bne	neon_simple_align_13_\a
	vext.8 \dst, \a, \b, #12
	b	neon_simple_align_end_\a

neon_simple_align_13_\a:
	cmp	\amount, #13
	bne	neon_simple_align_14_\a
	vext.8	\dst, \a, \b, #13
	b	neon_simple_align_end_\a

neon_simple_align_14_\a:
	cmp	\amount, #14
	bne	neon_simple_align_15_\a
	vext.8	\dst, \a, \b, #14
	b	neon_simple_align_end_\a

neon_simple_align_15_\a:
	vext.8	\dst, \a, \b, #15
	b	neon_simple_align_end_\a

neon_simple_align_end_\a:

.endm

.global adler32_vector
.balign 32
.type adler32_vector, %function

.fnstart
adler32_vector:
	push {r4-r11, lr}

	ldr		var_s1, [s1_ptr]
	ldr		var_s2, [s2_ptr]

	ldr		v_nmax, =VNMAX

	@load vord
	adr		tmp, adler_vord_const
	vld1.8	{d0, d1}, [tmp]!

	ldr		len, [len_addr]

	cmp	len, #32
	blo	adler32_vector_exit

	ands	var_f, buf_addr, #0xf
	rsbs	var_n, var_f, #16
	ands	buf_addr, buf_addr, #0xfffffff0 

	mla		var_s2, var_n, var_s1,	var_s2

	vmov.u32	vs1, #0
	vmov.u32	vs2, #0

	cmp		len, v_nmax		@len < VNMAX?
	movlo	var_k, len
	movhs	var_k, v_nmax 

	sub		len, len, var_k 

	@insert scalar start somewhere
	vmov.32	d4[0], var_s1
	vmov.32	d6[0], var_s2

	@get input data
	vld1.8	{d8, d9}, [buf_addr]
	
	@mask out excess data
	vmov.u8	v0, #0

	NEON_SIMPLE_ALIGN	in16, in16, v0, var_f

	NEON_SIMPLE_ALIGN	vord_a, vord, v0, var_f

	@pairwise add bytes and long
	vpaddl.u8	vtmp, in16
	vpadal.u16	vs1, vtmp 

	@apply order
	vmull.u8	vtmp, d8, d2
	vmlal.u8	vtmp, d9, d3
	vpadal.u16	vs2, vtmp

	add	buf_addr, buf_addr, #16
	sub	var_k, var_k, var_n

	cmp	var_k, #16
	blo	adler32_k_not_zero

adler32_k_ge_16:
	vmov.u32	vs1_r, #0

adler32_k_ge_16_loop:
	vmov.u16	vs2_lo, #0
	vmov.u16	vs2_hi, #0

	lsr	var_j, var_k, #4
	cmp var_j, #16
	movhs	var_j, #16

	sub	var_k, var_k, var_j, lsl #4

adler32_k_ge_16_loop1:
	vld1.8	{d8, d9}, [buf_addr]!

	vadd.i32	vs1_r, vs1_r, vs1
	vpaddl.u8	vtmp, in16
	vpadal.u16	vs1, vtmp

	vmlal.u8	vs2_lo, d8, d0
	vmlal.u8	vs2_hi, d9, d1

	subs	var_j, var_j, #1
	bne		adler32_k_ge_16_loop1

adler32_k_ge_16_loop1_exit:
	vpadal.u16	vs2, vs2_lo
	vpadal.u16	vs2, vs2_hi

	cmp var_k, #16
	bhs	adler32_k_ge_16_loop

adler32_k_ge_16_loop_exit:

	@reduce vs1 round sum before multiplying by 16
	VECTOR_REDUCE	vs1_r
	vshl.i32	vtmp, vs1_r, #4
	vadd.i32	vs2, vs2, vtmp

	VECTOR_REDUCE	vs2
	VECTOR_REDUCE	vs1

	add	len, len, var_k
	cmp	len, v_nmax

	movlo	var_k, len
	movhs	var_k, v_nmax 

	sub	len, len, var_k

	cmp	var_k, #16
	bhs	adler32_k_ge_16

adler32_k_not_zero:
	cmp var_k, #0
	beq	adler32_k_not_zero_exit

	rsbs	var_f, var_k, #16
	vdup.32	vdup_k, var_k
	vmla.i32	vs2, vs1, vdup_k

	vld1.8	{d8, d9}, [buf_addr], var_k
	NEON_SIMPLE_ALIGN	in16, v0, in16, var_k

	vpaddl.u8	vtmp, in16
	vpadal.u16	vs1, vtmp

	vmull.u8	vtmp, d8, d0
	vmlal.u8	vtmp, d9, d1
	vpadal.u16	vs2, vtmp

adler32_k_not_zero_exit:
	vpadd.i32	v_tsum, d5, d4
	vpadd.i32	v_tsum, v_tsum, v_tsum
	vmov.32		var_s1, v_tsum[0]
	vpadd.i32	v_tsum, d7, d6
	vpadd.i32	v_tsum, v_tsum, v_tsum
	vmov.32		var_s2, v_tsum[0]
	
	str	var_s1, [s1_ptr]
	str	var_s2, [s2_ptr]

adler32_vector_exit:
	str	len, [len_addr]
	pop	{r4-r11, pc}

.fnend





